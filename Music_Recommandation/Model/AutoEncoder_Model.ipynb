{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import imageio\n",
    "from scipy import misc\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图片集存放路径 \n",
    "img_path = '../MusicInputFig_Eng/*' \n",
    "imgs = glob.glob(img_path) #读取该路径下所有图片 返回路径列表list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(f):\n",
    "    x = misc.imread(f, mode='RGB')\n",
    "    return x.astype(np.float32) / 255 * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模块 载入图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/bob/.conda/envs/pythonDL/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "1500it [00:38, 39.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#得到训练集  注意: 这步很耗内存\n",
    "img_train = []\n",
    "index = 0\n",
    "for imgPath in tqdm(iter(imgs[:1500])):\n",
    "    fname = os.path.splitext(imgPath.split('/')[-1])[0] #获得歌曲名\n",
    "    index = index+1\n",
    "    img = imread(imgPath)\n",
    "    img_train.append(img)\n",
    "    del img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式训练 载入图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#得到训练集  注意: 这步很耗内存\n",
    "img_train = []\n",
    "index = 0\n",
    "with open('../Data/nameIndex.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for imgPath in tqdm(iter(imgs[:1500])):\n",
    "        fname = os.path.splitext(imgPath.split('/')[-1])[0] #获得歌曲名\n",
    "        row = [index, fname]\n",
    "        writer.writerow(row)\n",
    "        index = index+1\n",
    "        img = imread(imgPath)\n",
    "        img_train.append(img)\n",
    "        del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/.conda/envs/pythonDL/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "MusicName = 'The Glitch Mob,Mako,The Word Alive - RISE.mp3'\n",
    "\n",
    "MusicDir = '../testMusic/music/'\n",
    "FigDir = '../testMusic/musicFig/'\n",
    "absolutelyMusicDir = '/home/bob/Desktop/Music_Recommend/testMusic/music/'\n",
    "musicDir = '/home/bob/Music/music_dataset/'\n",
    "\n",
    "musicPath = MusicDir + MusicName\n",
    "musicFigPath = FigDir + MusicName\n",
    "FigPath = musicFigPath + '.png'\n",
    "#create spectrum\n",
    "#figPlot.pltSave(musicPath, musicFigPath)\n",
    "#read spectrum as array\n",
    "img = imread(FigPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.array(img_train)  \n",
    "x_train = image_train\n",
    "del img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1501, 256, 2560, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2560, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = x_train.shape[1]\n",
    "img_weight = x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 2560\n"
     ]
    }
   ],
   "source": [
    "print(img_height, img_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 128 # 隐变量维度\n",
    "alpha = 0.5 # 全局互信息的loss比重\n",
    "beta = 1.5 # 局部互信息的loss比重\n",
    "gamma = 0.01 # 先验分布的loss比重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器（卷积与最大池化）\n",
    "x_in = Input(shape=(img_height, img_weight, 3))\n",
    "x = x_in\n",
    "\n",
    "for i in range(4):\n",
    "    x = Conv2D(int(z_dim / 2**(3-i)), \n",
    "               kernel_size=(3,3), padding='SAME')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "feature_map = x # 截断到这里，认为到这里是feature_map（局部特征）\n",
    "feature_map_encoder = Model(x_in, x)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    x = Conv2D(z_dim,\n",
    "               kernel_size=(3,3),\n",
    "               padding='SAME')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "x = GlobalMaxPooling2D()(x) # 全局特征\n",
    "\n",
    "z_mean = Dense(z_dim)(x) # 均值，也就是最终输出的编码\n",
    "z_log_var = Dense(z_dim)(x) # 方差，这里都是模仿VAE的\n",
    "\n",
    "\n",
    "encoder = Model(x_in, z_mean) # 总的编码器就是输出z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501/1501 [==============================] - 53s 36ms/step\n",
      "0.0027704667\n",
      "0.07766766\n"
     ]
    }
   ],
   "source": [
    "# 输出编码器的特征\n",
    "#100\n",
    "zs = encoder.predict(x_train, verbose=True)\n",
    "print(zs.mean()) # 查看均值（简单观察先验分布有没有达到效果）\n",
    "print(zs.std()) # 查看方差（简单观察先验分布有没有达到效果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/bob/Desktop/MusicVec.txt', zs, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n",
      "0.0027704667\n",
      "0.07766766\n"
     ]
    }
   ],
   "source": [
    "# 输出编码器的特征\n",
    "#128\n",
    "z1s = encoder.predict(np.expand_dims(x_train[-1], axis=0), verbose=True)\n",
    "print(zs.mean()) # 查看均值（简单观察先验分布有没有达到效果）\n",
    "print(zs.std()) # 查看方差（简单观察先验分布有没有达到效果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "z2s = encoder.predict(np.expand_dims(img, axis=0), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 24s 16ms/step\n",
      "-0.014910269\n",
      "0.091249675\n"
     ]
    }
   ],
   "source": [
    "# 输出编码器的特征\n",
    "#135\n",
    "zs = encoder.predict(x_train, verbose=True)\n",
    "print(zs.mean()) # 查看均值（简单观察先验分布有没有达到效果）\n",
    "print(zs.std()) # 查看方差（简单观察先验分布有没有达到效果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重参数技巧\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    u = K.random_normal(shape=K.shape(z_mean))\n",
    "    return z_mean + K.exp(z_log_var / 2) * u\n",
    "\n",
    "# 重参数层，相当于给输入加入噪声\n",
    "z_samples = Lambda(sampling)([z_mean, z_log_var])\n",
    "prior_kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "\n",
    "# shuffle层，打乱第一个轴\n",
    "def shuffling(x):\n",
    "    idxs = K.arange(0, K.shape(x)[0])\n",
    "    idxs = K.tf.random_shuffle(idxs)\n",
    "    return K.gather(x, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 16, 160, 512), (None, 1, 10, 512)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-13f11514f758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mz_samples_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mz_samples_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_samples_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mz_f_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_samples_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mz_f_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_samples_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map_shuffle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 16, 160, 512), (None, 1, 10, 512)]"
     ]
    }
   ],
   "source": [
    "# 与随机采样的特征拼接（全局）\n",
    "z_shuffle = Lambda(shuffling)(z_samples)\n",
    "z_z_1 = Concatenate()([z_samples, z_samples])\n",
    "z_z_2 = Concatenate()([z_samples, z_shuffle])\n",
    "\n",
    "# 与随机采样的特征拼接（局部）\n",
    "feature_map_shuffle = Lambda(shuffling)(feature_map)\n",
    "z_samples_repeat = RepeatVector(16 * 160)(z_samples)\n",
    "z_samples_map = Reshape((16, 160, z_dim))(z_samples_repeat)\n",
    "z_f_1 = Concatenate()([z_samples_map, feature_map])\n",
    "z_f_2 = Concatenate()([z_samples_map, feature_map_shuffle])\n",
    "\n",
    "# 全局判别器\n",
    "z_in = Input(shape=(z_dim*2,))\n",
    "z = z_in\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "GlobalDiscriminator = Model(z_in, z)\n",
    "\n",
    "z_z_1_scores = GlobalDiscriminator(z_z_1)\n",
    "z_z_2_scores = GlobalDiscriminator(z_z_2)\n",
    "global_info_loss = - K.mean(K.log(z_z_1_scores + 1e-6) + K.log(1 - z_z_2_scores + 1e-6))\n",
    "\n",
    "# 局部判别器\n",
    "z_in = Input(shape=(None, None, z_dim*2))\n",
    "z = z_in\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(z_dim, activation='relu')(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "LocalDiscriminator = Model(z_in, z)\n",
    "\n",
    "z_f_1_scores = LocalDiscriminator(z_f_1)\n",
    "z_f_2_scores = LocalDiscriminator(z_f_2)\n",
    "local_info_loss = - K.mean(K.log(z_f_1_scores + 1e-6) + K.log(1 - z_f_2_scores + 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程  如果没有训练好的模型文件 则运行以下代码 若有模型文件 跳过下面两个代码块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 用来训练的模型\n",
    "model_train = Model(x_in, [z_z_1_scores, z_z_2_scores, z_f_1_scores, z_f_2_scores])\n",
    "model_train.add_loss(alpha * global_info_loss + beta * local_info_loss + gamma * prior_kl_loss)\n",
    "model_train.compile(optimizer=Adam(1e-3))\n",
    "\n",
    "model_train.fit(x_train, epochs=100, batch_size=100)\n",
    "#model_train.save_weights('image.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入训练好的模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来训练的模型\n",
    "model_train = Model(x_in, [z_z_1_scores, z_z_2_scores, z_f_1_scores, z_f_2_scores])\n",
    "model_train.add_loss(alpha * global_info_loss + beta * local_info_loss + gamma * prior_kl_loss)\n",
    "model_train.compile(optimizer=Adam(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 2560, 3)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 2560, 32)     896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 2560, 32)     128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256, 2560, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 1280, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 1280, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 1280, 64)     256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128, 1280, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 640, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 640, 128)      73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 640, 128)      512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64, 640, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 320, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 320, 256)      295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 320, 256)      1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 320, 256)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 160, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 160, 256)      590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 160, 256)      1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 160, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 160, 256)      590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 160, 256)      1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 160, 256)      0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "=================================================================\n",
      "Total params: 1,638,336\n",
      "Trainable params: 1,636,352\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_train, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_train.save_weights('Music_Recommend.weights')\n",
    "#model_train.save('Music_Model_100.h5')\n",
    "model_train = model_train.load_weights('Music_Recommend.weights', by_name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b7420a512268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(model_train), type(history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model_train\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['epochs'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zs 就是编码器根据输入得出的每个图片的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../Data/MusicVec.txt', zs, delimiter=',')\n",
    "#zs = np.loadtxt('../Data/MusicVec.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下部分以迁移至Music_Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#参宿为输出图片路径 这里是指当前路径下pic文件夹下test.jpg\n",
    "#sample_knn('pic/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def chooseSimMus():\n",
    "    musInputList = [236, 344, 407, 426, 744, 214, 1977, 1675, 1535, 1371]\n",
    "    n = len(musInputList)\n",
    "    #n = 5\n",
    "    topn = 10\n",
    "    #figure1 = np.zeros((img_height*n, img_weight*topn, 3))\n",
    "    #figure2 = np.zeros((img_height*n, img_weight*topn, 3))\n",
    "    zs_ = zs / (zs**2).sum(1, keepdims=True)**0.5\n",
    "    similar_list = []\n",
    "    for i , one in zip(range(n), musInputList):\n",
    "        #one = 0 #测试选择0号\n",
    "        #one = np.random.choice(len(x_train)) #随机选择一张图的序号\n",
    "        idxs = ((zs**2).sum(1) + (zs[one]**2).sum() - 2 * np.dot(zs, zs[one])).argsort()[:topn]\n",
    "        similar = [one, idxs.tolist()]\n",
    "        similar_list.append(similar)\n",
    "    return similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSimMus():\n",
    "    #musInputList = [236, 344, 407, 426, 744, 214, 1977, 1675, 1535, 1371]\n",
    "    n = 30\n",
    "    topn = 11\n",
    "    #figure1 = np.zeros((img_height*n, img_weight*topn, 3))\n",
    "    #figure2 = np.zeros((img_height*n, img_weight*topn, 3))\n",
    "    zs_ = zs / (zs**2).sum(1, keepdims=True)**0.5\n",
    "    similar_list = []\n",
    "    for i in range(n):\n",
    "        one = np.random.choice(len(x_train)) #随机选择一张图的序号\n",
    "        idxs = ((zs**2).sum(1) + (zs[one]**2).sum() - 2 * np.dot(zs, zs[one])).argsort()[:topn]\n",
    "        similar = [one, idxs.tolist()]\n",
    "        similar_list.append(similar)\n",
    "    return similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar_list = chooseSimMus()\n",
    "similar_list = randomSimMus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lena = mpimg.imread('../Data/music_l2.png')\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[880, [880, 1442, 457, 771, 1162, 656, 254, 80, 320, 1014, 1337]],\n",
       " [546, [546, 110, 975, 1128, 164, 268, 1047, 705, 1191, 1105, 187]],\n",
       " [585, [585, 1138, 545, 471, 1148, 822, 1433, 568, 403, 623, 152]],\n",
       " [605, [605, 678, 885, 711, 1139, 1187, 326, 491, 733, 98, 753]],\n",
       " [544, [544, 300, 1079, 109, 767, 1236, 1214, 115, 436, 961, 219]],\n",
       " [1063, [1063, 836, 1250, 1470, 1265, 1366, 381, 1093, 385, 1236, 387]],\n",
       " [490, [490, 965, 225, 659, 939, 896, 543, 33, 587, 1025, 180]],\n",
       " [895, [895, 143, 192, 908, 122, 736, 510, 1176, 807, 1104, 526]],\n",
       " [835, [835, 91, 1456, 893, 851, 331, 899, 1057, 653, 18, 607]],\n",
       " [1260, [1260, 205, 268, 40, 581, 1128, 980, 1080, 258, 1227, 144]],\n",
       " [1344, [1344, 1411, 1025, 1388, 570, 535, 502, 1378, 1180, 534, 870]],\n",
       " [1463, [1463, 643, 27, 1112, 970, 1152, 163, 1333, 1139, 414, 768]],\n",
       " [975, [975, 1191, 1105, 546, 1047, 705, 94, 110, 220, 1435, 267]],\n",
       " [935, [935, 689, 296, 1421, 859, 262, 873, 410, 105, 1343, 951]],\n",
       " [778, [778, 1438, 991, 907, 461, 502, 910, 707, 1000, 1180, 98]],\n",
       " [747, [747, 1001, 16, 736, 845, 949, 269, 1370, 620, 704, 406]],\n",
       " [209, [209, 1090, 1009, 842, 385, 43, 245, 1243, 88, 295, 615]],\n",
       " [274, [274, 1401, 207, 342, 116, 1368, 54, 243, 1049, 1403, 1199]],\n",
       " [1439, [1439, 846, 773, 909, 78, 1130, 527, 1347, 1010, 1075, 383]],\n",
       " [1481, [1481, 831, 95, 770, 155, 133, 1433, 728, 970, 1117, 137]],\n",
       " [302, [302, 413, 1480, 1116, 1475, 1106, 250, 879, 1470, 1296, 1091]],\n",
       " [905, [905, 1173, 1098, 1468, 310, 163, 1255, 1176, 1337, 1021, 507]],\n",
       " [1142, [1142, 631, 963, 676, 155, 304, 1356, 572, 977, 39, 455]],\n",
       " [1236, [1236, 1470, 602, 914, 1229, 123, 1029, 571, 698, 728, 631]],\n",
       " [1194, [1194, 1059, 873, 242, 1337, 225, 746, 286, 1486, 896, 422]],\n",
       " [167, [167, 503, 917, 409, 45, 501, 1076, 49, 133, 367, 383]],\n",
       " [468, [468, 1296, 1271, 1429, 691, 1292, 1014, 734, 831, 1314, 1029]],\n",
       " [470, [470, 110, 851, 331, 722, 1483, 164, 1128, 1213, 607, 651]],\n",
       " [1449, [1449, 1136, 1450, 645, 45, 1457, 359, 990, 238, 271, 1161]],\n",
       " [672, [672, 484, 726, 897, 1196, 838, 1235, 1444, 825, 676, 49]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{880: [880, 1442, 457, 771, 1162, 656, 254, 80, 320, 1014, 1337],\n",
       " 546: [546, 110, 975, 1128, 164, 268, 1047, 705, 1191, 1105, 187],\n",
       " 585: [585, 1138, 545, 471, 1148, 822, 1433, 568, 403, 623, 152],\n",
       " 605: [605, 678, 885, 711, 1139, 1187, 326, 491, 733, 98, 753],\n",
       " 544: [544, 300, 1079, 109, 767, 1236, 1214, 115, 436, 961, 219],\n",
       " 1063: [1063, 836, 1250, 1470, 1265, 1366, 381, 1093, 385, 1236, 387],\n",
       " 490: [490, 965, 225, 659, 939, 896, 543, 33, 587, 1025, 180],\n",
       " 895: [895, 143, 192, 908, 122, 736, 510, 1176, 807, 1104, 526],\n",
       " 835: [835, 91, 1456, 893, 851, 331, 899, 1057, 653, 18, 607],\n",
       " 1260: [1260, 205, 268, 40, 581, 1128, 980, 1080, 258, 1227, 144],\n",
       " 1344: [1344, 1411, 1025, 1388, 570, 535, 502, 1378, 1180, 534, 870],\n",
       " 1463: [1463, 643, 27, 1112, 970, 1152, 163, 1333, 1139, 414, 768],\n",
       " 975: [975, 1191, 1105, 546, 1047, 705, 94, 110, 220, 1435, 267],\n",
       " 935: [935, 689, 296, 1421, 859, 262, 873, 410, 105, 1343, 951],\n",
       " 778: [778, 1438, 991, 907, 461, 502, 910, 707, 1000, 1180, 98],\n",
       " 747: [747, 1001, 16, 736, 845, 949, 269, 1370, 620, 704, 406],\n",
       " 209: [209, 1090, 1009, 842, 385, 43, 245, 1243, 88, 295, 615],\n",
       " 274: [274, 1401, 207, 342, 116, 1368, 54, 243, 1049, 1403, 1199],\n",
       " 1439: [1439, 846, 773, 909, 78, 1130, 527, 1347, 1010, 1075, 383],\n",
       " 1481: [1481, 831, 95, 770, 155, 133, 1433, 728, 970, 1117, 137],\n",
       " 302: [302, 413, 1480, 1116, 1475, 1106, 250, 879, 1470, 1296, 1091],\n",
       " 905: [905, 1173, 1098, 1468, 310, 163, 1255, 1176, 1337, 1021, 507],\n",
       " 1142: [1142, 631, 963, 676, 155, 304, 1356, 572, 977, 39, 455],\n",
       " 1236: [1236, 1470, 602, 914, 1229, 123, 1029, 571, 698, 728, 631],\n",
       " 1194: [1194, 1059, 873, 242, 1337, 225, 746, 286, 1486, 896, 422],\n",
       " 167: [167, 503, 917, 409, 45, 501, 1076, 49, 133, 367, 383],\n",
       " 468: [468, 1296, 1271, 1429, 691, 1292, 1014, 734, 831, 1314, 1029],\n",
       " 470: [470, 110, 851, 331, 722, 1483, 164, 1128, 1213, 607, 651],\n",
       " 1449: [1449, 1136, 1450, 645, 45, 1457, 359, 990, 238, 271, 1161],\n",
       " 672: [672, 484, 726, 897, 1196, 838, 1235, 1444, 825, 676, 49]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_dic = {}\n",
    "for img_id, name_list in similar_list:\n",
    "    similar_dic[img_id] = name_list\n",
    "similar_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similar_arr = pd.DataFrame(similar_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_arr.to_csv('../Data/SimilaryMusicIndex.csv', index=False)\n",
    "similar_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ind_data = pd.read_csv('../Data/SimilaryMusicIndex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarImgs = img_ind_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_data = pd.read_csv('../Data/nameIndex.csv', names=['id', 'name'])\n",
    "img_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img_id in tarImgs:\n",
    "    targetImg = img_data[img_data['id']==int(img_id)]\n",
    "    targetImgName = targetImg['name'].values[0]\n",
    "    simImgs = img_ind_data[img_id].values.tolist()\n",
    "    print(\"Input Image name is:\"+'\\n', targetImgName)\n",
    "    print('Similar Image name is:')\n",
    "    for sims in simImgs:\n",
    "        #print(sims)\n",
    "        simImage = img_data[img_data['id']==int(sims)]\n",
    "        simName = simImage['name'].values[0]\n",
    "        print('\\t' + os.path.join('/home/bob/Music/music_dataset', simName))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
